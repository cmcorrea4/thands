import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import time
import os
import pandas as pd
import pickle
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Configuraci√≥n de la p√°gina
st.set_page_config(page_title="Clasificador de Posiciones de Manos", page_icon="üëã")
st.title("Clasificador de Posiciones de Manos")

# Ruta espec√≠fica del proyecto
PROJECT_PATH = r"C:\Users\cmcor\Manos"
DATA_PATH = os.path.join(PROJECT_PATH, "data")
MODELS_PATH = os.path.join(PROJECT_PATH, "models")

# Crear directorios para guardar datos y modelos
if not os.path.exists(PROJECT_PATH):
    os.makedirs(PROJECT_PATH)
if not os.path.exists(DATA_PATH):
    os.makedirs(DATA_PATH)
if not os.path.exists(MODELS_PATH):
    os.makedirs(MODELS_PATH)

# Inicializar MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# Inicializar variables de estado
if 'features_data' not in st.session_state:
    st.session_state.features_data = []
if 'label' not in st.session_state:
    st.session_state.label = ""
if 'active' not in st.session_state:
    st.session_state.active = False
if 'counter' not in st.session_state:
    st.session_state.counter = 0
if 'start_button' not in st.session_state:
    st.session_state.start_button = False

# Funci√≥n para extraer caracter√≠sticas de los puntos clave de la mano
def extract_hand_features(hand_landmarks, handedness):
    """Extrae caracter√≠sticas relevantes de los puntos clave de la mano"""
    try:
        # Convertir landmarks a un arreglo NumPy
        hand_points = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])
        
        # Usar la mu√±eca como punto de referencia para normalizar
        reference_point = hand_points[mp_hands.HandLandmark.WRIST][:2]  # Solo x, y
        
        # Extraer caracter√≠sticas normalizadas
        features = []
        
        # 1. Coordenadas normalizadas relativas a la mu√±eca
        for i, landmark in enumerate(hand_points):
            # Normalizar coordenadas x, y
            norm_x = landmark[0] - reference_point[0]
            norm_y = landmark[1] - reference_point[1]
            # Incluir z para profundidad relativa
            norm_z = landmark[2]
            features.extend([norm_x, norm_y, norm_z])
        
        # 2. Calcular distancias entre puntos espec√≠ficos
        def calculate_distance_3d(p1, p2):
            """Calcula la distancia euclidiana 3D entre dos puntos"""
            return np.linalg.norm(np.array(p1) - np.array(p2))
        
        # Distancias entre dedos
        thumb_tip = hand_points[mp_hands.HandLandmark.THUMB_TIP]
        index_tip = hand_points[mp_hands.HandLandmark.INDEX_FINGER_TIP]
        middle_tip = hand_points[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]
        ring_tip = hand_points[mp_hands.HandLandmark.RING_FINGER_TIP]
        pinky_tip = hand_points[mp_hands.HandLandmark.PINKY_TIP]
        
        # Distancias entre puntas de dedos adyacentes
        thumb_index_dist = calculate_distance_3d(thumb_tip, index_tip)
        index_middle_dist = calculate_distance_3d(index_tip, middle_tip)
        middle_ring_dist = calculate_distance_3d(middle_tip, ring_tip)
        ring_pinky_dist = calculate_distance_3d(ring_tip, pinky_tip)
        
        # Distancias a la mu√±eca
        thumb_wrist_dist = calculate_distance_3d(thumb_tip, hand_points[mp_hands.HandLandmark.WRIST])
        
        features.extend([thumb_index_dist, index_middle_dist, middle_ring_dist, 
                        ring_pinky_dist, thumb_wrist_dist])
        
        # 3. Calcular √°ngulos entre articulaciones de dedos
        def calculate_angle(a, b, c):
            """Calcula el √°ngulo entre tres puntos"""
            a = np.array(a)
            b = np.array(b)
            c = np.array(c)
            
            # Vectores
            ba = a - b
            bc = c - b
            
            # Calcular coseno del √°ngulo
            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))
            angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))
            
            return np.degrees(angle)
        
        # √Ångulos de cada dedo (MCP-PIP-DIP-TIP)
        for finger_id in [mp_hands.HandLandmark.THUMB_MCP, mp_hands.HandLandmark.INDEX_FINGER_MCP,
                         mp_hands.HandLandmark.MIDDLE_FINGER_MCP, mp_hands.HandLandmark.RING_FINGER_MCP,
                         mp_hands.HandLandmark.PINKY_MCP]:
            base_id = finger_id
            if finger_id != mp_hands.HandLandmark.THUMB_MCP:
                # Dedos normales
                mcp = hand_points[base_id]
                pip = hand_points[base_id + 1]
                dip = hand_points[base_id + 2]
                tip = hand_points[base_id + 3]
                
                # √Ångulos de cada articulaci√≥n
                pip_angle = calculate_angle(mcp, pip, dip)
                dip_angle = calculate_angle(pip, dip, tip)
                
                features.extend([pip_angle, dip_angle])
            else:
                # Pulgar
                cmc = hand_points[mp_hands.HandLandmark.THUMB_CMC]
                mcp = hand_points[mp_hands.HandLandmark.THUMB_MCP]
                ip = hand_points[mp_hands.HandLandmark.THUMB_IP]
                tip = hand_points[mp_hands.HandLandmark.THUMB_TIP]
                
                mcp_angle = calculate_angle(cmc, mcp, ip)
                ip_angle = calculate_angle(mcp, ip, tip)
                
                features.extend([mcp_angle, ip_angle])
        
        # 4. Indicador de qu√© mano es (izquierda o derecha)
        hand_side = 1 if handedness.classification[0].label == "Right" else 0
        features.append(hand_side)
        
        return np.array(features)
    except Exception as e:
        st.error(f"Error al extraer caracter√≠sticas: {str(e)}")
        return None

# Funci√≥n para procesar video
def process_video(mode="collect"):
    """Procesa video de la c√°mara web usando OpenCV directamente"""
    # Inicializar c√°mara
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        st.error("No se pudo abrir la c√°mara. Verifica que est√© conectada y disponible.")
        return
    
    # Inicializar MediaPipe Hands
    with mp_hands.Hands(
        max_num_hands=2,  # Detectar hasta 2 manos
        min_detection_confidence=0.7,
        min_tracking_confidence=0.7,
        model_complexity=1  # 0 o 1
    ) as hands:
        
        # Para predicci√≥n
        prediction = "N/A"
        if mode == "predict" and os.path.exists(os.path.join(MODELS_PATH, 'svm_model.pkl')):
            model = pickle.load(open(os.path.join(MODELS_PATH, 'svm_model.pkl'), 'rb'))
            scaler = pickle.load(open(os.path.join(MODELS_PATH, 'scaler.pkl'), 'rb'))
            classes = pickle.load(open(os.path.join(MODELS_PATH, 'classes.pkl'), 'rb'))
        
        # Variables para control de tiempo
        last_time = time.time()
        
        # Placeholder para la imagen
        img_placeholder = st.empty()
        
        # Indicador de estado
        status_text = st.empty()
        
        running = True
        while running and st.session_state.start_button:
            # Leer frame
            ret, img = cap.read()
            if not ret:
                st.error("No se pudo leer el frame de la c√°mara.")
                break
            
            # Voltear horizontalmente la imagen (efecto espejo)
            img = cv2.flip(img, 1)
            
            # Convertir a RGB para MediaPipe
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # Procesar imagen
            results = hands.process(img_rgb)
            
            # Visualizaci√≥n
            visual_img = img.copy()
            
            # Si se detectan manos
            if results.multi_hand_landmarks:
                for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
                    # Dibujar los landmarks de la mano
                    mp_drawing.draw_landmarks(
                        visual_img,
                        hand_landmarks,
                        mp_hands.HAND_CONNECTIONS,
                        mp_drawing_styles.get_default_hand_landmarks_style(),
                        mp_drawing_styles.get_default_hand_connections_style()
                    )
                    
                    # Extraer caracter√≠sticas
                    features = extract_hand_features(hand_landmarks, handedness)
                    
                    if features is not None:
                        # MODO RECOLECCI√ìN
                        if mode == "collect" and st.session_state.active:
                            current_time = time.time()
                            # Tomar datos cada 0.3 segundos
                            if current_time - last_time > 0.3:
                                # Guardar en estado
                                st.session_state.features_data.append(features.tolist())
                                st.session_state.counter += 1
                                last_time = current_time
                                
                                # Actualizar informaci√≥n de estado
                                status_text.text(f"Muestras recolectadas: {st.session_state.counter}")
                        
                        # MODO PREDICCI√ìN
                        elif mode == "predict" and 'model' in locals():
                            current_time = time.time()
                            if current_time - last_time > 0.5:
                                # Predecir
                                features_reshaped = features.reshape(1, -1)
                                scaled_features = scaler.transform(features_reshaped)
                                prediction = model.predict(scaled_features)[0]
                                last_time = current_time
                                
                                # Actualizar informaci√≥n de estado
                                status_text.text(f"Predicci√≥n: {prediction}")
                                
                                # Mostrar predicci√≥n en la imagen
                                cv2.putText(
                                    visual_img,
                                    f"{handedness.classification[0].label} - {prediction}",
                                    (10, 30),
                                    cv2.FONT_HERSHEY_SIMPLEX,
                                    1,
                                    (0, 255, 255),
                                    2
                                )
            
            # Mostrar informaci√≥n en pantalla
            if mode == "collect":
                # Estado
                status = "ACTIVO ‚úÖ" if st.session_state.active else "INACTIVO ‚ùå"
                cv2.putText(
                    visual_img,
                    f"Estado: {status}",
                    (20, visual_img.shape[0] - 80),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.8,
                    (0, 255, 0) if st.session_state.active else (0, 0, 255),
                    2
                )
                
                # Etiqueta
                cv2.putText(
                    visual_img,
                    f"Etiqueta: {st.session_state.label}",
                    (20, visual_img.shape[0] - 50),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.8,
                    (255, 255, 0),
                    2
                )
                
                # Contador
                cv2.putText(
                    visual_img,
                    f"Muestras: {st.session_state.counter}",
                    (20, visual_img.shape[0] - 20),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.8,
                    (0, 165, 255),
                    2
                )
            
            # Convertir a RGB para mostrar en Streamlit
            visual_img_rgb = cv2.cvtColor(visual_img, cv2.COLOR_BGR2RGB)
            
            # Mostrar imagen
            img_placeholder.image(visual_img_rgb, channels="RGB")
            
            # Comprobar si se debe detener
            if not st.session_state.start_button:
                running = False
        
        # Liberar la c√°mara
        cap.release()

# Funci√≥n para guardar datos (sin modificaciones)
def save_data(label, features):
    """Guarda caracter√≠sticas en CSV"""
    if not features or len(features) == 0:
        return False
    
    try:
        # Crear DataFrame
        df = pd.DataFrame(features)
        
        # A√±adir etiqueta
        df['label'] = label
        
        # Guardar CSV con ruta absoluta
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        filename = os.path.join(DATA_PATH, f"{label}_{timestamp}.csv")
        
        # Guardar con modo de escritura expl√≠cito
        df.to_csv(filename, index=False, mode='w')
        
        # Log para depuraci√≥n
        print(f"Guardado archivo: {filename}")
        print(f"Contenido: {len(df)} filas, {df.shape[1]} columnas")
        
        return filename
    except Exception as e:
        st.error(f"Error al guardar: {str(e)}")
        return None

# Funci√≥n para entrenar modelo (sin modificaciones)
def train_model():
    """Entrena modelo SVM con datos guardados"""
    files = [f for f in os.listdir(DATA_PATH) if f.endswith('.csv')]
    
    if not files:
        return "No hay archivos de datos"
    
    try:
        # Cargar datos
        datasets = []
        for file in files:
            file_path = os.path.join(DATA_PATH, file)
            df = pd.read_csv(file_path)
            datasets.append(df)
        
        # Combinar datos
        data = pd.concat(datasets, ignore_index=True)
        
        # Separar caracter√≠sticas y etiquetas
        X = data.drop('label', axis=1)
        y = data['label']
        
        # Normalizar
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Entrenar SVM
        model = SVC(kernel='rbf', C=10, gamma='scale')
        model.fit(X_scaled, y)
        
        # Guardar modelo con ruta absoluta
        pickle.dump(model, open(os.path.join(MODELS_PATH, 'svm_model.pkl'), 'wb'))
        pickle.dump(scaler, open(os.path.join(MODELS_PATH, 'scaler.pkl'), 'wb'))
        pickle.dump(list(model.classes_), open(os.path.join(MODELS_PATH, 'classes.pkl'), 'wb'))
        
        return f"Modelo entrenado con {len(data)} muestras"
    except Exception as e:
        return f"Error al entrenar: {str(e)}"

# Funci√≥n para iniciar/detener c√°mara (sin modificaciones)
def toggle_camera():
    st.session_state.start_button = not st.session_state.start_button

# Interfaz principal
st.sidebar.header("Modo")
app_mode = st.sidebar.radio(
    "Seleccionar modo",
    ["Recolecci√≥n", "Entrenamiento", "Predicci√≥n"]
)

# Mostrar rutas de directorio
st.sidebar.subheader("Rutas")
st.sidebar.info(f"Proyecto: {PROJECT_PATH}\nDatos: {DATA_PATH}\nModelos: {MODELS_PATH}")

# MODO RECOLECCI√ìN
if app_mode == "Recolecci√≥n":
    st.header("Recolecci√≥n de Datos")
    
    # Formulario de etiqueta
    st.session_state.label = st.text_input("Etiqueta para posici√≥n de mano:", st.session_state.label, 
                                          placeholder="Ejemplo: pu√±o, palma_abierta, pulgar_arriba")
    
    # Instrucciones
    st.markdown("""
    ### üìã Instrucciones
    1. Escribe una etiqueta descriptiva para la posici√≥n de mano que quieres capturar
    2. Aseg√∫rate de estar en un lugar con buena iluminaci√≥n
    3. Coloca la mano a una distancia adecuada para que sea claramente visible
    4. Presiona "Iniciar C√°mara" y luego "Iniciar Recolecci√≥n"
    5. Recolecta al menos 50-100 muestras para cada posici√≥n
    
    > üí° **Consejo**: Para obtener un mejor modelo, var√≠a ligeramente el √°ngulo y la posici√≥n de la mano mientras recopilas datos.
    """)
    
    # Botones de control
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        # Bot√≥n para iniciar/detener c√°mara
        if st.button("Iniciar/Detener C√°mara"):
            toggle_camera()
    with col2:
        if st.button("Iniciar Recolecci√≥n"):
            st.session_state.active = True
    with col3:
        if st.button("Detener Recolecci√≥n"):
            st.session_state.active = False
    with col4:
        if st.button("Reiniciar Contador"):
            st.session_state.features_data = []
            st.session_state.counter = 0
    
    # Iniciar procesamiento de video si se ha pulsado el bot√≥n
    if st.session_state.start_button:
        process_video(mode="collect")
    
    # Informaci√≥n de recolecci√≥n
    st.info(f"Datos recolectados: {st.session_state.counter} muestras")
    
    # Bot√≥n para guardar
    if st.button("Guardar Datos"):
        if st.session_state.features_data and len(st.session_state.features_data) > 0:
            filename = save_data(st.session_state.label, st.session_state.features_data)
            if filename:
                st.success(f"Datos guardados en: {filename}")
                st.session_state.features_data = []
                st.session_state.counter = 0
            else:
                st.error("Error al guardar datos")
        else:
            st.warning("No hay datos para guardar")
    
    # Lista de archivos
    st.subheader("Archivos existentes")
    files = [f for f in os.listdir(DATA_PATH) if f.endswith('.csv')]
    if files:
        for file in files:
            file_path = os.path.join(DATA_PATH, file)
            try:
                df = pd.read_csv(file_path)
                st.text(f"{file}: {len(df)} muestras, Etiqueta: {df['label'].iloc[0]}")
            except Exception as e:
                st.text(f"{file}: Error al leer ({str(e)})")
    else:
        st.text("No hay archivos guardados")

# MODO ENTRENAMIENTO
elif app_mode == "Entrenamiento":
    st.header("Entrenamiento de Modelo SVM")
    
    # Informaci√≥n de datos
    st.subheader("Datos disponibles")
    files = [f for f in os.listdir(DATA_PATH) if f.endswith('.csv')]
    
    if files:
        data_info = {}
        total = 0
        
        for file in files:
            file_path = os.path.join(DATA_PATH, file)
            try:
                df = pd.read_csv(file_path)
                label = df['label'].iloc[0]
                count = len(df)
                total += count
                
                if label in data_info:
                    data_info[label] += count
                else:
                    data_info[label] = count
            except Exception as e:
                st.warning(f"Error al leer {file}: {str(e)}")
        
        # Mostrar resumen
        st.write(f"Total: {total} muestras")
        st.write("Distribuci√≥n:")
        for label, count in data_info.items():
            st.write(f"- {label}: {count} muestras ({count/total*100:.1f}%)")
        
        # Recomendaciones
        st.info("""
        ### üí° Recomendaciones para mejores resultados:
        - Aseg√∫rate de tener al menos 50-100 muestras por posici√≥n de mano
        - La distribuci√≥n de clases debe ser equilibrada (similar n√∫mero de muestras para cada posici√≥n)
        - Para mayor precisi√≥n, recolecta datos en diferentes condiciones de iluminaci√≥n y √°ngulos
        """)
        
        # Bot√≥n para entrenar
        if st.button("Entrenar Modelo"):
            with st.spinner("Entrenando..."):
                result = train_model()
                st.success(result)
    else:
        st.warning("No hay datos disponibles")

# MODO PREDICCI√ìN
else:
    st.header("Predicci√≥n de Posiciones de Manos")
    
    # Comprobar modelo
    model_path = os.path.join(MODELS_PATH, 'svm_model.pkl')
    if os.path.exists(model_path):
        # Cargar clases
        classes = pickle.load(open(os.path.join(MODELS_PATH, 'classes.pkl'), 'rb'))
        st.write(f"Modelo cargado. Posiciones reconocidas: {', '.join(classes)}")
        
        # Bot√≥n para iniciar/detener c√°mara
        if st.button("Iniciar/Detener C√°mara"):
            toggle_camera()
            
        # Iniciar procesamiento de video si se ha pulsado el bot√≥n
        if st.session_state.start_button:
            process_video(mode="predict")
            
        st.info("Muestra tus manos frente a la c√°mara para ver la predicci√≥n")
        
        # Consejos
        st.markdown("""
        ### üìã Consejos para la predicci√≥n:
        1. Aseg√∫rate de que tus manos sean claramente visibles en la c√°mara
        2. Mant√©n buena iluminaci√≥n para una detecci√≥n precisa
        3. Intenta replicar las posiciones con las que entrenaste el modelo
        4. Mant√©n la posici√≥n estable durante unos segundos
        5. El sistema puede detectar hasta 2 manos simult√°neamente
        """)
    else:
        st.warning(f"No hay modelo entrenado disponible en {model_path}")
        st.info("Primero debes recolectar datos y entrenar un modelo en las secciones anteriores.")

# Informaci√≥n de estado
st.sidebar.subheader("Estado")
st.sidebar.info(f"""
- Modo: {app_mode}
- Etiqueta: {st.session_state.label}
- Muestras: {st.session_state.counter}
- Recolecci√≥n: {"Activa" if st.session_state.active else "Inactiva"}
- C√°mara: {"Activa" if st.session_state.start_button else "Inactiva"}
""")

# Sugerencias de posiciones de manos
st.sidebar.subheader("Ejemplos de Posiciones")
st.sidebar.markdown("""
### Gestos comunes:
- Palma abierta
- Pu√±o cerrado
- Pulgar arriba
- OK (pulgar e √≠ndice)
- Paz (V con √≠ndice y medio)
- Rock and roll (cuernos)
- Mano plana
- Puntero (dedo √≠ndice)
- Pistola
- N√∫meros (1-5 dedos)

### Posiciones espec√≠ficas:
- Se√±alar hacia abajo
- Garra
- Posici√≥n de relajaci√≥n
- Mano en forma de taza
""")
